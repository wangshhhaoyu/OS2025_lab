# Lab8 文件系统实验报告

## 练习1：完成读文件操作的实现（需要编码）

### 1.1 实验要求

首先了解打开文件的处理流程，然后参考本实验后续的文件读写操作的过程分析，填写在 `kern/fs/sfs/sfs_inode.c` 中的 `sfs_io_nolock()` 函数，实现读文件中数据的代码。

### 1.2 设计思路

`sfs_io_nolock` 函数用于读/写文件内容，需要处理三种情况：

1. **第一个不完整块**：如果读取的起始位置 `offset` 不在块边界上，需要先处理从 `offset` 到第一个块末尾的数据
2. **中间的完整块**：对于对齐的完整块，可以直接进行整块读写
3. **最后一个不完整块**：如果结束位置不在块边界上，需要处理最后一个块的部分数据

### 1.3 代码实现

```c
static int
sfs_io_nolock(struct sfs_fs *sfs, struct sfs_inode *sin, void *buf, off_t offset, size_t *alenp, bool write) {
    struct sfs_disk_inode *din = sin->din;
    assert(din->type != SFS_TYPE_DIR);
    off_t endpos = offset + *alenp, blkoff;
    *alenp = 0;
    
    // 参数检查
    if (offset < 0 || offset >= SFS_MAX_FILE_SIZE || offset > endpos) {
        return -E_INVAL;
    }
    if (offset == endpos) {
        return 0;
    }
    if (endpos > SFS_MAX_FILE_SIZE) {
        endpos = SFS_MAX_FILE_SIZE;
    }
    if (!write) {
        if (offset >= din->size) {
            return 0;
        }
        if (endpos > din->size) {
            endpos = din->size;
        }
    }

    // 根据读/写操作选择相应的函数
    int (*sfs_buf_op)(struct sfs_fs *sfs, void *buf, size_t len, uint32_t blkno, off_t offset);
    int (*sfs_block_op)(struct sfs_fs *sfs, void *buf, uint32_t blkno, uint32_t nblks);
    if (write) {
        sfs_buf_op = sfs_wbuf, sfs_block_op = sfs_wblock;
    }
    else {
        sfs_buf_op = sfs_rbuf, sfs_block_op = sfs_rblock;
    }

    int ret = 0;
    size_t size, alen = 0;
    uint32_t ino;
    uint32_t blkno = offset / SFS_BLKSIZE;          // 起始块号
    uint32_t nblks = endpos / SFS_BLKSIZE - blkno;  // 完整块数量

    // (1) 处理第一个不完整块
    blkoff = offset % SFS_BLKSIZE;
    if (blkoff != 0) {
        size = (nblks != 0) ? (SFS_BLKSIZE - blkoff) : (endpos - offset);
        if ((ret = sfs_bmap_load_nolock(sfs, sin, blkno, &ino)) != 0) {
            goto out;
        }
        if ((ret = sfs_buf_op(sfs, buf, size, ino, blkoff)) != 0) {
            goto out;
        }
        alen += size;
        if (nblks == 0) {
            goto out;  // 整个读写都在一个块内
        }
        buf += size;
        blkno++;
        nblks--;
    }

    // (2) 处理中间的完整块
    while (nblks > 0) {
        if ((ret = sfs_bmap_load_nolock(sfs, sin, blkno, &ino)) != 0) {
            goto out;
        }
        if ((ret = sfs_block_op(sfs, buf, ino, 1)) != 0) {
            goto out;
        }
        alen += SFS_BLKSIZE;
        buf += SFS_BLKSIZE;
        blkno++;
        nblks--;
    }

    // (3) 处理最后一个不完整块
    size = endpos % SFS_BLKSIZE;
    if (size != 0) {
        if ((ret = sfs_bmap_load_nolock(sfs, sin, blkno, &ino)) != 0) {
            goto out;
        }
        if ((ret = sfs_buf_op(sfs, buf, size, ino, 0)) != 0) {
            goto out;
        }
        alen += size;
    }

out:
    *alenp = alen;
    if (offset + alen > sin->din->size) {
        sin->din->size = offset + alen;
        sin->dirty = 1;
    }
    return ret;
}
```

### 1.4 关键函数说明

| 函数 | 功能 |
|------|------|
| `sfs_bmap_load_nolock` | 根据文件内的块索引，获取对应的磁盘块号 |
| `sfs_rbuf` / `sfs_wbuf` | 读/写不完整的块（非对齐操作） |
| `sfs_rblock` / `sfs_wblock` | 读/写完整的块（对齐操作） |

---

## 练习2：完成基于文件系统的执行程序机制的实现（需要编码）

### 2.1 实验要求

改写 `proc.c` 中的 `load_icode` 函数和其他相关函数，实现基于文件系统的执行程序机制。

### 2.2 设计思路

与 Lab5 不同，Lab8 的 `load_icode` 需要从文件系统中读取 ELF 可执行文件，而不是从内存中读取。主要步骤：

1. 创建新的内存管理结构 `mm_struct`
2. 创建新的页表
3. 从文件中读取 ELF 头和程序头
4. 根据程序头加载各个段（TEXT/DATA/BSS）
5. 设置用户栈
6. 设置命令行参数
7. 设置 trapframe 以便返回用户态

### 2.3 代码实现

首先在 `alloc_proc` 中初始化 `filesp`：

```c
static struct proc_struct *
alloc_proc(void) {
    struct proc_struct *proc = kmalloc(sizeof(struct proc_struct));
    if (proc != NULL) {
        // ... 其他初始化代码 ...
        
        // LAB8: 初始化文件结构指针
        proc->filesp = NULL;
    }
    return proc;
}
```

然后实现 `load_icode` 函数：

```c
static int
load_icode(int fd, int argc, char **kargv) {
    assert(argc >= 0 && argc <= EXEC_MAX_ARG_NUM);

    // (1) 创建新的 mm_struct
    struct mm_struct *mm;
    if ((mm = mm_create()) == NULL) {
        goto bad_mm;
    }

    // (2) 创建新的页表
    if (setup_pgdir(mm) != 0) {
        goto bad_pgdir_cleanup_mm;
    }

    // (3) 加载 ELF 文件的各个段
    struct Page *page;
    
    // (3.1) 读取 ELF 头
    struct elfhdr __elf, *elf = &__elf;
    if ((load_icode_read(fd, elf, sizeof(struct elfhdr), 0)) != 0) {
        goto bad_elf_cleanup_pgdir;
    }
    
    // 检查 ELF 魔数
    if (elf->e_magic != ELF_MAGIC) {
        goto bad_elf_cleanup_pgdir;
    }

    // (3.2) 遍历程序头
    struct proghdr __ph, *ph = &__ph;
    uint32_t vm_flags, perm, phnum;
    for (phnum = 0; phnum < elf->e_phnum; phnum++) {
        off_t phoff = elf->e_phoff + sizeof(struct proghdr) * phnum;
        if ((load_icode_read(fd, ph, sizeof(struct proghdr), phoff)) != 0) {
            goto bad_cleanup_mmap;
        }
        if (ph->p_type != ELF_PT_LOAD) {
            continue;
        }
        if (ph->p_filesz > ph->p_memsz) {
            goto bad_cleanup_mmap;
        }
        
        // (3.3) 建立虚拟内存映射
        vm_flags = 0;
        perm = PTE_U;
        if (ph->p_flags & ELF_PF_X) vm_flags |= VM_EXEC;
        if (ph->p_flags & ELF_PF_W) vm_flags |= VM_WRITE;
        if (ph->p_flags & ELF_PF_R) vm_flags |= VM_READ;
        if (vm_flags & VM_WRITE) perm |= PTE_W;
        if (vm_flags & VM_READ) perm |= PTE_R;
        if (vm_flags & VM_EXEC) perm |= PTE_X;
        
        if ((mm_map(mm, ph->p_va, ph->p_memsz, vm_flags, NULL)) != 0) {
            goto bad_cleanup_mmap;
        }
        
        // (3.4) 分配页面并从文件读取内容
        off_t offset = ph->p_offset;
        size_t off, size;
        uintptr_t start = ph->p_va, end, la = ROUNDDOWN(start, PGSIZE);

        end = ph->p_va + ph->p_filesz;
        while (start < end) {
            if ((page = pgdir_alloc_page(mm->pgdir, la, perm)) == NULL) {
                goto bad_cleanup_mmap;
            }
            off = start - la;
            size = PGSIZE - off;
            la += PGSIZE;
            if (end < la) {
                size -= la - end;
            }
            if ((load_icode_read(fd, page2kva(page) + off, size, offset)) != 0) {
                goto bad_cleanup_mmap;
            }
            start += size;
            offset += size;
        }
        
        // (3.5) 为 BSS 段分配页面并清零
        end = ph->p_va + ph->p_memsz;
        if (start < la) {
            if (start == end) continue;
            off = start + PGSIZE - la;
            size = PGSIZE - off;
            if (end < la) size -= la - end;
            memset(page2kva(page) + off, 0, size);
            start += size;
        }
        while (start < end) {
            if ((page = pgdir_alloc_page(mm->pgdir, la, perm)) == NULL) {
                goto bad_cleanup_mmap;
            }
            off = start - la;
            size = PGSIZE - off;
            la += PGSIZE;
            if (end < la) size -= la - end;
            memset(page2kva(page) + off, 0, size);
            start += size;
        }
    }
    
    sysfile_close(fd);

    // (4) 设置用户栈
    vm_flags = VM_READ | VM_WRITE | VM_STACK;
    if ((mm_map(mm, USTACKTOP - USTACKSIZE, USTACKSIZE, vm_flags, NULL)) != 0) {
        goto bad_cleanup_mmap;
    }
    assert(pgdir_alloc_page(mm->pgdir, USTACKTOP - PGSIZE, PTE_USER) != NULL);
    assert(pgdir_alloc_page(mm->pgdir, USTACKTOP - 2 * PGSIZE, PTE_USER) != NULL);
    assert(pgdir_alloc_page(mm->pgdir, USTACKTOP - 3 * PGSIZE, PTE_USER) != NULL);
    assert(pgdir_alloc_page(mm->pgdir, USTACKTOP - 4 * PGSIZE, PTE_USER) != NULL);

    // (5) 设置当前进程的 mm 和页表
    mm_count_inc(mm);
    current->mm = mm;
    current->pgdir = PADDR(mm->pgdir);
    lsatp(PADDR(mm->pgdir));

    // (6) 设置用户栈中的命令行参数
    uint32_t argv_size = 0;
    int i;
    for (i = 0; i < argc; i++) {
        argv_size += strnlen(kargv[i], EXEC_MAX_ARG_LEN + 1) + 1;
    }

    uintptr_t stacktop = USTACKTOP - (argv_size / sizeof(long) + 1) * sizeof(long);
    char **uargv = (char **)(stacktop - argc * sizeof(char *));

    argv_size = 0;
    for (i = 0; i < argc; i++) {
        uargv[i] = strcpy((char *)(stacktop + argv_size), kargv[i]);
        argv_size += strnlen(kargv[i], EXEC_MAX_ARG_LEN + 1) + 1;
    }

    stacktop = (uintptr_t)uargv - sizeof(int);
    *(int *)stacktop = argc;

    // (7) 设置 trapframe
    struct trapframe *tf = current->tf;
    uintptr_t sstatus = read_csr(sstatus);
    sstatus &= ~SSTATUS_SPP;   // 返回用户态
    sstatus |= SSTATUS_SPIE;   // 开中断
    sstatus &= ~SSTATUS_SIE;
    
    memset(tf, 0, sizeof(struct trapframe));
    tf->gpr.sp = stacktop;
    tf->epc = elf->e_entry;
    tf->status = sstatus;

    return 0;

bad_cleanup_mmap:
    exit_mmap(mm);
bad_elf_cleanup_pgdir:
    put_pgdir(mm);
bad_pgdir_cleanup_mm:
    mm_destroy(mm);
bad_mm:
    return -E_NO_MEM;
}
```

### 2.4 与 Lab5 的区别

| 对比项 | Lab5 | Lab8 |
|--------|------|------|
| 程序来源 | 内存中的二进制数据 | 文件系统中的 ELF 文件 |
| 读取方式 | 直接内存拷贝 | 通过 `load_icode_read` 读取 |
| 文件描述符 | 无 | 需要 fd 参数 |
| 关闭文件 | 无 | 需要 `sysfile_close(fd)` |

---

## 扩展练习 Challenge1：完成基于"UNIX的PIPE机制"的设计方案

### 3.1 设计目标

在 ucore 中实现 UNIX 风格的管道（Pipe）机制，支持进程间通过管道进行单向数据传输。

### 3.2 数据结构设计

```c
#define PIPE_BUF_SIZE 4096  // 管道缓冲区大小

// 管道结构体
struct pipe {
    char buffer[PIPE_BUF_SIZE];  // 环形缓冲区
    uint32_t read_pos;           // 读指针位置
    uint32_t write_pos;          // 写指针位置
    uint32_t count;              // 缓冲区中的数据量
    
    semaphore_t read_sem;        // 读信号量（用于阻塞读进程）
    semaphore_t write_sem;       // 写信号量（用于阻塞写进程）
    semaphore_t mutex;           // 互斥锁
    
    int read_open;               // 读端是否打开
    int write_open;              // 写端是否打开
    int ref_count;               // 引用计数
};

// 管道文件描述符类型
#define FD_PIPE_READ  3   // 管道读端
#define FD_PIPE_WRITE 4   // 管道写端
```

### 3.3 接口设计

```c
// 创建管道，返回两个文件描述符
// pipefd[0] 为读端，pipefd[1] 为写端
int sys_pipe(int pipefd[2]);

// 从管道读取数据
ssize_t pipe_read(struct pipe *p, void *buf, size_t n);

// 向管道写入数据
ssize_t pipe_write(struct pipe *p, const void *buf, size_t n);

// 关闭管道的一端
int pipe_close(struct pipe *p, int is_write_end);
```

### 3.4 同步互斥处理

1. **写满阻塞**：当缓冲区满时，写进程在 `write_sem` 上等待
2. **读空阻塞**：当缓冲区空时，读进程在 `read_sem` 上等待
3. **互斥访问**：使用 `mutex` 保护对缓冲区的并发访问

```c
ssize_t pipe_read(struct pipe *p, void *buf, size_t n) {
    down(&p->mutex);
    while (p->count == 0) {
        if (!p->write_open) {
            up(&p->mutex);
            return 0;  // 写端关闭，返回 EOF
        }
        up(&p->mutex);
        down(&p->read_sem);  // 等待数据
        down(&p->mutex);
    }
    
    // 从环形缓冲区读取数据
    size_t bytes_read = 0;
    while (bytes_read < n && p->count > 0) {
        ((char *)buf)[bytes_read++] = p->buffer[p->read_pos];
        p->read_pos = (p->read_pos + 1) % PIPE_BUF_SIZE;
        p->count--;
    }
    
    up(&p->write_sem);  // 唤醒可能等待的写进程
    up(&p->mutex);
    return bytes_read;
}
```

---

## 扩展练习 Challenge2：完成基于"UNIX的软连接和硬连接机制"的设计方案

### 4.1 硬连接（Hard Link）

#### 4.1.1 概念

硬连接是文件系统中多个目录项指向同一个 inode 的机制。

#### 4.1.2 数据结构

```c
// 在 sfs_disk_inode 中已有 nlinks 字段
struct sfs_disk_inode {
    uint32_t size;
    uint16_t type;
    uint16_t nlinks;    // 硬连接计数
    uint32_t blocks;
    uint32_t direct[SFS_NDIRECT];
    uint32_t indirect;
};
```

#### 4.1.3 接口设计

```c
// 创建硬连接
// old_path: 原文件路径
// new_path: 新链接路径
int sys_link(const char *old_path, const char *new_path);

// 删除链接
int sys_unlink(const char *path);
```

#### 4.1.4 实现要点

```c
int sfs_link(struct inode *dir, const char *name, struct inode *target) {
    // 1. 检查目标是否为目录（不允许对目录创建硬连接）
    if (target->din->type == SFS_TYPE_DIR) {
        return -E_ISDIR;
    }
    
    // 2. 在目录中创建新的目录项，指向同一个 inode
    // 3. 增加 inode 的 nlinks 计数
    target->din->nlinks++;
    target->dirty = 1;
    
    return 0;
}

int sfs_unlink(struct inode *dir, const char *name) {
    // 1. 查找目录项
    // 2. 减少 nlinks 计数
    // 3. 如果 nlinks == 0，释放 inode 及其数据块
    // 4. 删除目录项
}
```

### 4.2 软连接（Symbolic Link）

#### 4.2.1 概念

软连接是一种特殊的文件，其内容是另一个文件的路径名。

#### 4.2.2 数据结构

```c
// 新增文件类型
#define SFS_TYPE_LINK 3  // 符号链接

// 符号链接的 inode，其数据块存储目标路径
struct sfs_disk_inode {
    uint32_t size;       // 目标路径的长度
    uint16_t type;       // SFS_TYPE_LINK
    uint16_t nlinks;
    uint32_t blocks;
    uint32_t direct[SFS_NDIRECT];  // 存储目标路径
    uint32_t indirect;
};
```

#### 4.2.3 接口设计

```c
// 创建符号链接
int sys_symlink(const char *target, const char *linkpath);

// 读取符号链接的目标路径
ssize_t sys_readlink(const char *path, char *buf, size_t bufsiz);
```

#### 4.2.4 实现要点

```c
int sfs_symlink(struct sfs_fs *sfs, struct inode *dir, 
                const char *name, const char *target) {
    // 1. 创建新的 inode，类型为 SFS_TYPE_LINK
    // 2. 将目标路径写入 inode 的数据块
    // 3. 在目录中创建目录项
}

// 路径解析时需要处理符号链接
int sfs_lookup(struct inode *dir, char *path, struct inode **node_store) {
    // 如果遇到符号链接，读取其目标路径
    // 递归解析目标路径（需要检测循环链接）
}
```

#### 4.2.5 同步互斥处理

1. **创建/删除链接**：需要获取目录的写锁
2. **修改 nlinks**：需要获取 inode 的锁
3. **路径解析**：需要处理符号链接循环（设置最大递归深度）

```c
#define MAX_SYMLINK_DEPTH 8  // 最大符号链接递归深度

int resolve_path(const char *path, struct inode **result, int depth) {
    if (depth > MAX_SYMLINK_DEPTH) {
        return -E_LOOP;  // 检测到循环链接
    }
    // ... 解析路径 ...
    if (inode->din->type == SFS_TYPE_LINK) {
        // 读取目标路径并递归解析
        return resolve_path(target_path, result, depth + 1);
    }
}
```

---

## 实验结果

运行 `make qemu` 后，系统成功启动并进入 shell：

```
user sh is running!!!
$ hello
Hello world!!.
I am process 3.
hello pass.
$ exit
I am the parent. Forking the child...
I am parent, fork a child pid 5
I am the parent, waiting now..
I am the child.
waitpid 5 ok.
exit pass.
$ forktest
I am child 0
I am child 1
...
I am child 31
forktest pass.
```

---

## 实验总结

本次实验主要完成了：

1. **文件读取操作**：实现了 `sfs_io_nolock` 函数，处理了块对齐和非对齐的读写情况
2. **程序加载机制**：实现了基于文件系统的 `load_icode` 函数，从 SFS 文件系统加载 ELF 可执行文件
3. **设计方案**：完成了 UNIX 管道机制和软/硬连接机制的设计

通过本次实验，深入理解了：
- 文件系统的层次结构（VFS → SFS → 设备驱动）
- 块设备的读写操作和缓冲管理
- ELF 文件的加载过程
- 进程间通信机制的设计思想
